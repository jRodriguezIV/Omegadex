{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c27def0",
   "metadata": {},
   "source": [
    "# AWS Data Services and the Modern Data Architecture # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636bb2f",
   "metadata": {},
   "source": [
    "Within organizations, teams often build data solutions in isolation and have their own data ingestion, storage, management, and governance layers.  This leads to delays and increases the cost of data-driven decisions. It also prevents the deeper insights that come when an organization analyzes all their relevant data together.\n",
    "\n",
    "To avoid these challenges, you must build a modern data architecture for analytics and insights that breaks down all data silosâ€”including third-party data. This architecture creates end-to-end governance and puts the data in the hands of everyone in the organization.\n",
    "\n",
    "A modern data strategy uses technology building blocks to help you manage, access, analyze, and act on your data. \n",
    "\n",
    "The building blocks roughly correspond to a basic workflow. Choose the numbered markers in the following diagram for descriptions of each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8633c8bd",
   "metadata": {},
   "source": [
    "- **Ingest**: Data of various types and from multiple sources is ingested into the system.\n",
    "- **Store**: Data is stored in a centralized repository rather than in departmental silos. \n",
    "- **Catalog**: Data is cataloged, indexed, and tagged to facilitate search and retrieval.\n",
    "- **Process**: Much of the data in raw form is not usable for analysis. In the process stage, data is transformed into more usable formats\n",
    "- **Deliver for consumption**: Processed data is delivered to consumers and stakeholders for visualization and analysis.\n",
    "- **Security and governance**: The entire data analytics system depends on the following:\n",
    "    - Security of data in transit and at rest\n",
    "    - Controlled access by authorized users only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5bb208",
   "metadata": {},
   "source": [
    "## Data lake ##\n",
    "Store stage of the basic workflow.\n",
    "Before you can ingest data, you need a place to put it, therefore a modern data architecture starts with the data lake. A data lake is a centralized repository that you can use store structured, semi-structured, and unstructured data at scale. Organizations can use it to ingest, store, and analyze diverse datasets without the need for extensive preprocessing. \n",
    "\n",
    "\n",
    "- Amazon S3 provides an optimal foundation for a data lake because of its virtually unlimited scalability and high durability. You can seamlessly and non-disruptively increase storage from gigabytes to petabytes of content and only pay for what you use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d545034e",
   "metadata": {},
   "source": [
    "## Specialized ingest services ##\n",
    "Ingest stage of the basic workflow.\n",
    "After you have established the data lake, you can use specialized AWS services to ingest different types of data into your data lake.\n",
    "\n",
    "- Use **AWS DMS** to load data from relational and non-relational databases.\n",
    "- Ingest real-time data streams with **Amazon Data Firehose**. Convert your data stream into formats such as Apache Parquet or ORC, decompress the data, or perform custom data transformations.\n",
    "- With **Amazon MSK**, build fully managed Apache Kafka clusters for real-time streaming data pipelines and applications.\n",
    "- Connect billions of IoT devices and route trillions of messages to AWS services with **AWS IoT Core**.\n",
    "- Use **AWS DataSync** to transfer data from on-premises file shares, object storage systems, and Hadoop clusters to AWS storage services. Synchronize on a scheduled basis.\n",
    "- Automate file transfers into and out of Amazon S3 using SFTP, FTPS, and SFTP protocols with the **Transfer Family**.\n",
    "- In cases where transfer through a network is not feasible because of data volume or sensitivity, use the **AWS Snow Family** of purpose-built physical devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6966b66",
   "metadata": {},
   "source": [
    "## Cataloging service ## \n",
    "Catalog stage of the basic workflow.\n",
    "An essential component of a data lake built on Amazon S3 is the data catalog. Organizations can use cataloging to keep track of data assets and understand what data exists, where it is located, its quality, and how it is used. A data catalog is designed to provide a single source of truth about the contents of the data lake. \n",
    "\n",
    "\n",
    "- AWS Glue Data Catalog creates a catalog of metadata about your stored assets. Use this catalog to help search and find relevant data sources based on various attributes like name, owner, business terms, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15e83c",
   "metadata": {},
   "source": [
    "## Processing services ##\n",
    "Process stage of the basic workflow.\n",
    "After the data is cataloged, it can now be processed or transformed into formats that are more useful for analysis and insights. Transformation can include data type conversion, filtering, aggregation, standardization, and normalizing.\n",
    "\n",
    "\n",
    "- Use **AWS Glue**, a fully managed extract, transform, and load (ETL) service, to prepare and cleanse data from various sources for analysis. It helps classify data, extract schema, and populate data catalogs.\n",
    "\n",
    "\n",
    "- With **Amazon EMR**, process big datasets using open-source frameworks, customized Amazon Elastic Compute Cloud (Amazon EC2) clusters, Amazon Elastic Kubernetes Service (Amazon EKS), AWS Outposts, or Amazon EMR Serverless. It can be used to run batch jobs for data processing.\n",
    "\n",
    "\n",
    "- Quickly author SQL code for real-time data processing using **Amazon Managed Service for Apache Flink**. Tasks include filtering, aggregating, joining, and deriving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b18498f",
   "metadata": {},
   "source": [
    "## Analytics services ##\n",
    "Deliver stage of the basic workflow.\n",
    "Transformed data is delivered to consumers and stakeholders, such as data scientists, data analysts, and business analysts. The primary purpose of data analytics is to extract insights from data that can lead to good business or organizational outcomes. Many AWS services can be used at this stage.\n",
    "\n",
    "\n",
    "- **Amazon Redshift** can directly analyze large sets of structured data across many functional databases and datasets without moving the data.\n",
    "\n",
    "\n",
    "- **Athena** queries large datasets directly on Amazon S3 using standard SQL syntax, in various formats such as CSV, Parquet, and ORC.\n",
    "\n",
    "\n",
    "- Use **Amazon EMR** to run analytics frameworks like Apache Spark, Hive, Presto, and Flink on large datasets stored in AWS services like Amazon S3 and Amazon DynamoDB. Examples include log analysis, machine learning, data science, web indexing and scientific simulations.\n",
    "\n",
    "\n",
    "- Use more than fourteen purpose-built **Amazon databases** to store, query, and analyze large datasets. Choose from relational, key-value, document, in-memory, graph, time series, wide column, and ledger databases.\n",
    "\n",
    "\n",
    "- Deploy, operate, and scale **OpenSearch** clusters in the AWS Cloud. Analyze large volumes of data from various sources like Amazon Kinesis Data Streams, Amazon S3, and Amazon DynamoDB using the OpenSearch APIs.\n",
    "\n",
    "\n",
    "- Visualize and analyze large datasets using SQL, charts, graphs, and dashboards with **Amazon QuickSight**.\n",
    "\n",
    "\n",
    "- Build, train, and deploy machine learning models for use in predictive analytics, computer vision for image recognition, natural language processing, recommendation systems, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25396c0",
   "metadata": {},
   "source": [
    "## Security and governance ##\n",
    "Security and governance in the basic workflow.\n",
    "\n",
    "\n",
    "- **Security** in data analytics systems refers to measures taken to protect data from unauthorized access, breaches, or attacks. It involves safeguarding data confidentiality, integrity, and availability. The entire data analytics system depends on data being secured and accessible only by authorized users.\n",
    "\n",
    "- **Governance** encompasses the policies, procedures, and processes that ensure the proper management, quality, and use of data. It involves defining roles, responsibilities, and decision-making processes related to data.\n",
    "\n",
    "- Following are some of the AWS services used for security and governance. These are covered in more detail in this course in the **Security and Monitoring in Data Analytics Systems** lesson.\n",
    "- With **Lake Formation**, you can centrally manage and scale fine-grained data access permissions and share data with confidence within and outside your organization.\n",
    "\n",
    "- **IAM** manages fine-grained access and permissions for human users, software users, other services, and microservices.\n",
    "- Use **AWS KMS** to create and control data encryption keys for data at rest and in transit\n",
    "- Use **Macie** to automatically discover, classify, and protect sensitive data in AWS, such as personally identifiable information (PII).\n",
    "- Use **Amazon DataZone** to catalog, discover, share, and govern data stored across AWS, on premises, and third-party sources.\n",
    "- **Audit Manager** continuously audits usage to assess risk and compliance with regulations and industry standards."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0389660",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
